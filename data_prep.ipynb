{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11df56f6-4a91-4cf5-a1c6-fff2a14b349c",
   "metadata": {},
   "source": [
    "# Data Prep Notebook\n",
    "### Purpose is to download, process, and write out instruction tuning examples for various ciphers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f957f460-d90d-4d7e-8643-3b23975f2bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "from transformers import T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1148a79a-1086-4dfd-b5bd-3528bd520b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enciphering/deciphering helpers\n",
    "char_to_num = {\n",
    "    'a': 0,\n",
    "    'b': 1,\n",
    "    'c': 2,\n",
    "    'd': 3,\n",
    "    'e': 4,\n",
    "    'f': 5,\n",
    "    'g': 6,\n",
    "    'h': 7,\n",
    "    'i': 8,\n",
    "    'j': 9,\n",
    "    'k': 10,\n",
    "    'l': 11,\n",
    "    'm': 12,\n",
    "    'n': 13,\n",
    "    'o': 14,\n",
    "    'p': 15,\n",
    "    'q': 16,\n",
    "    'r': 17,\n",
    "    's': 18,\n",
    "    't': 19,\n",
    "    'u': 20,\n",
    "    'v': 21,\n",
    "    'w': 22,\n",
    "    'x': 23,\n",
    "    'y': 24,\n",
    "    'z': 25,\n",
    "}\n",
    "\n",
    "\n",
    "# Remove all non alphabet text except spaces\n",
    "def format_text(text):\n",
    "    plaintext = re.sub(r'[^A-Za-z ]+', '', text)\n",
    "    return plaintext.lower()\n",
    "\n",
    "\n",
    "# NOTE: shift can be negative (left) or positive (right)\n",
    "# If encode=True, encipher text, otherwise decipher\n",
    "def caesar_cipher(original, shift, encode):\n",
    "    if encode:\n",
    "        myshift = shift\n",
    "    else:\n",
    "        myshift = shift * -1\n",
    "    newtext = ''\n",
    "    for i in original:\n",
    "        if i == ' ':  # Preserve spaces\n",
    "            newtext += ' '\n",
    "        else:\n",
    "            newnum = (char_to_num[i] + myshift) % 26\n",
    "            newchar = list(char_to_num.keys())[list(char_to_num.values()).index(newnum)]\n",
    "            newtext += newchar\n",
    "    return newtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "235befd6-2dea-4fea-8178-b5ba845eef80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|█| 4.40k/4.40k [00:00<00:00, 8.10MB/\n",
      "Downloading metadata: 100%|█████| 2.20k/2.20k [00:00<00:00, 13.4MB/s]\n",
      "Downloading readme: 100%|███████| 8.06k/8.06k [00:00<00:00, 30.4MB/s]\n",
      "Downloading data: 100%|███████████| 578M/578M [00:10<00:00, 57.3MB/s]\n",
      "Generating train split: 100%|█| 3803957/3803957 [00:53<00:00, 71639.3\n",
      "Generating validation split: 100%|█| 189651/189651 [00:02<00:00, 7315\n",
      "Generating test split: 100%|█| 1951/1951 [00:00<00:00, 70948.75 examp\n",
      "/opt/conda/envs/langgenv2/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "# Download gigaword dataset from huggingface\n",
    "DATA_NAME = \"gigaword\"\n",
    "gigaword = load_dataset(DATA_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce2e4167-f729-4c40-8ab8-6cb20c988b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['document', 'summary'],\n",
       "        num_rows: 3803957\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['document', 'summary'],\n",
       "        num_rows: 189651\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['document', 'summary'],\n",
       "        num_rows: 1951\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gigaword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54f638d2-fc76-44c2-8018-ac2008b42939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': \"australia 's current account deficit shrunk by a record #.## billion dollars -lrb- #.## billion us -rrb- in the june quarter due to soaring commodity prices , figures released monday showed .\",\n",
       " 'summary': 'australian current account deficit narrows sharply'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gigaword['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cfdd9501-acad-4cf6-9dca-9cbaabf42880",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"google/flan-t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "41401fff-b9c7-4f6d-9351-15ad9997e931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the preprocessing function\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Add prefix to the sentences, tokenize the text, and set the labels\"\"\"\n",
    "    # Create lists of data of instructions w/ ciphered text and the corresponding plaintext\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for doc in examples[\"document\"]:\n",
    "        shift = random.randint(-25, 25)\n",
    "        prefix = f\"Use a Caesar cipher with shift {shift} to decipher the following text: \"\n",
    "        text = format_text(doc)\n",
    "        inputs.append(prefix + caesar_cipher(text, shift, True))\n",
    "        targets.append(text)\n",
    "\n",
    "    # Tokenize\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True)\n",
    "    labels = tokenizer(text_target=targets, max_length=512, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e9854e46-2612-4a35-99f6-c18dfccb1019",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████| 3803957/3803957 [40:33<00:00, 1563.19 examples/s]\n",
      "Map: 100%|██████████| 189651/189651 [01:57<00:00, 1607.60 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = gigaword.map(preprocess_function, batched=True, remove_columns=[\"document\", \"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bc7b3273-0fe0-482d-8cef-fa9bf346442a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2048,\n",
       "  3,\n",
       "  9,\n",
       "  26218,\n",
       "  3,\n",
       "  3389,\n",
       "  760,\n",
       "  28,\n",
       "  4108,\n",
       "  3,\n",
       "  11590,\n",
       "  12,\n",
       "  20,\n",
       "  3389,\n",
       "  760,\n",
       "  8,\n",
       "  826,\n",
       "  1499,\n",
       "  10,\n",
       "  3,\n",
       "  208,\n",
       "  51,\n",
       "  115,\n",
       "  51,\n",
       "  172,\n",
       "  3,\n",
       "  15,\n",
       "  3,\n",
       "  172,\n",
       "  1824,\n",
       "  32,\n",
       "  3,\n",
       "  32,\n",
       "  9,\n",
       "  26,\n",
       "  115,\n",
       "  3,\n",
       "  51,\n",
       "  172,\n",
       "  102,\n",
       "  3,\n",
       "  122,\n",
       "  172,\n",
       "  210,\n",
       "  3,\n",
       "  32,\n",
       "  9,\n",
       "  63,\n",
       "  115,\n",
       "  122,\n",
       "  89,\n",
       "  1824,\n",
       "  26,\n",
       "  3,\n",
       "  32,\n",
       "  9,\n",
       "  26,\n",
       "  115,\n",
       "  1584,\n",
       "  3,\n",
       "  89,\n",
       "  17,\n",
       "  1824,\n",
       "  3,\n",
       "  122,\n",
       "  1000,\n",
       "  89,\n",
       "  1824,\n",
       "  102,\n",
       "  3,\n",
       "  15,\n",
       "  89,\n",
       "  51,\n",
       "  89,\n",
       "  1824,\n",
       "  15,\n",
       "  3,\n",
       "  15,\n",
       "  51,\n",
       "  413,\n",
       "  3,\n",
       "  23,\n",
       "  1824,\n",
       "  102,\n",
       "  172,\n",
       "  1824,\n",
       "  15,\n",
       "  2028,\n",
       "  157,\n",
       "  3,\n",
       "  89,\n",
       "  17,\n",
       "  1824,\n",
       "  157,\n",
       "  3,\n",
       "  17,\n",
       "  1167,\n",
       "  3,\n",
       "  51,\n",
       "  7,\n",
       "  26,\n",
       "  1824,\n",
       "  1824,\n",
       "  102,\n",
       "  3,\n",
       "  89,\n",
       "  9,\n",
       "  409,\n",
       "  76,\n",
       "  172,\n",
       "  3,\n",
       "  19042,\n",
       "  1824,\n",
       "  15,\n",
       "  3,\n",
       "  76,\n",
       "  172,\n",
       "  3,\n",
       "  15,\n",
       "  122,\n",
       "  115,\n",
       "  1824,\n",
       "  26,\n",
       "  32,\n",
       "  9,\n",
       "  63,\n",
       "  115,\n",
       "  1],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [2662,\n",
       "  2837,\n",
       "  3,\n",
       "  7,\n",
       "  9705,\n",
       "  11736,\n",
       "  11,\n",
       "  3,\n",
       "  6513,\n",
       "  1218,\n",
       "  11736,\n",
       "  13,\n",
       "  8,\n",
       "  18279,\n",
       "  2315,\n",
       "  243,\n",
       "  62,\n",
       "  26,\n",
       "  1496,\n",
       "  1135,\n",
       "  79,\n",
       "  141,\n",
       "  4686,\n",
       "  12,\n",
       "  1715,\n",
       "  3859,\n",
       "  16,\n",
       "  1355,\n",
       "  25750,\n",
       "  1085,\n",
       "  1]}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6ba6c2a5-5b1c-4ccf-b07b-0f2ee3b1efd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (8/8 shards): 100%|█| 3803957/3803957 [00:12<00:00\n",
      "Saving the dataset (1/1 shards): 100%|█| 189651/189651 [00:01<00:00, \n",
      "Saving the dataset (1/1 shards): 100%|█| 1951/1951 [00:00<00:00, 1446\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset.save_to_disk(dataset_dict_path='/home/as6734/langgen_class_project/data/caesar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eee1c11-8bbe-4592-8c8c-3eaa58b7f5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved datasets\n",
    "data_files = {\"train\": \"train.csv\", \"test\": \"caesar_test.csv\", \"validation\": \"caesar_validation.csv\"}\n",
    "dataset = load_dataset(\"caesar/\", data_dir='/home/as6734/langgen_class_project/data/caesar', data_files=data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "91072a88-47ef-47fa-8284-f8500f86e87d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ABCDEFG HIJK'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix = \"Use a Caesar cipher with shift -3 to decipher the following text: \"\n",
    "s = \"Use a Caesar cipher with shift -3 to decipher the following text: ABCDEFG HIJK\"\n",
    "s.replace(prefix, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a53d799-b1fd-4c88-8219-e9c61461e686",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
