{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11df56f6-4a91-4cf5-a1c6-fff2a14b349c",
   "metadata": {},
   "source": [
    "# Data Prep/Training/Evaluation Notebook for Reduced Caesar Cipher Shifts Seen in Training\n",
    "### Purpose is to download, process, and write out instruction tuning examples for Caesar ciphers\n",
    "### Then train a model without the full range of cipher shifts in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f957f460-d90d-4d7e-8643-3b23975f2bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "import random\n",
    "from datasets import load_from_disk, load_dataset\n",
    "from transformers import T5Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3910137e-7fdc-43f8-ba68-15f1967113ac",
   "metadata": {},
   "source": [
    "## Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1148a79a-1086-4dfd-b5bd-3528bd520b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enciphering/deciphering helpers\n",
    "char_to_num = {\n",
    "    'a': 0,\n",
    "    'b': 1,\n",
    "    'c': 2,\n",
    "    'd': 3,\n",
    "    'e': 4,\n",
    "    'f': 5,\n",
    "    'g': 6,\n",
    "    'h': 7,\n",
    "    'i': 8,\n",
    "    'j': 9,\n",
    "    'k': 10,\n",
    "    'l': 11,\n",
    "    'm': 12,\n",
    "    'n': 13,\n",
    "    'o': 14,\n",
    "    'p': 15,\n",
    "    'q': 16,\n",
    "    'r': 17,\n",
    "    's': 18,\n",
    "    't': 19,\n",
    "    'u': 20,\n",
    "    'v': 21,\n",
    "    'w': 22,\n",
    "    'x': 23,\n",
    "    'y': 24,\n",
    "    'z': 25,\n",
    "}\n",
    "\n",
    "\n",
    "# Remove all non alphabet text except spaces\n",
    "def format_text(text):\n",
    "    plaintext = re.sub(r'[^A-Za-z ]+', '', text)\n",
    "    return plaintext.lower()\n",
    "\n",
    "\n",
    "# NOTE: shift can be negative (left) or positive (right)\n",
    "# If encode=True, encipher text, otherwise decipher\n",
    "def caesar_cipher(original, shift, encode):\n",
    "    if encode:\n",
    "        myshift = shift\n",
    "    else:\n",
    "        myshift = shift * -1\n",
    "    newtext = ''\n",
    "    for i in original:\n",
    "        if i == ' ':  # Preserve spaces\n",
    "            newtext += ' '\n",
    "        else:\n",
    "            newnum = (char_to_num[i] + myshift) % 26\n",
    "            newchar = list(char_to_num.keys())[list(char_to_num.values()).index(newnum)]\n",
    "            newtext += newchar\n",
    "    return newtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "235befd6-2dea-4fea-8178-b5ba845eef80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/langgen/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['document', 'summary'],\n",
       "        num_rows: 3803957\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['document', 'summary'],\n",
       "        num_rows: 189651\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['document', 'summary'],\n",
       "        num_rows: 1951\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download gigaword dataset from huggingface\n",
    "DATA_NAME = \"gigaword\"\n",
    "gigaword = load_dataset(DATA_NAME)\n",
    "gigaword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54f638d2-fc76-44c2-8018-ac2008b42939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41a6b28494c44ce98abf9105b4377f63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3803957 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cf1cb72e39d4fb287b260323c68bc9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/189651 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49ba51d4727e4aa3a774294377f1be72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1951 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50e5bc7245a94fd5b8e9eed028d58f73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/38040 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['document', 'summary'],\n",
       "        num_rows: 38040\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['document', 'summary'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['document', 'summary'],\n",
       "        num_rows: 1951\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reduce the length of GIGAWORD dataset to make training faster\n",
    "small_dataset = gigaword.filter(lambda example, idx: idx < 38040, with_indices=True)\n",
    "small_dataset['validation'] = small_dataset['validation'].filter(lambda example, idx: idx < 2000, with_indices=True)\n",
    "small_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfdd9501-acad-4cf6-9dca-9cbaabf42880",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Get the base model tokenizer\n",
    "MODEL_NAME = \"google/flan-t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6dcd3b9-c10b-4883-b793-a254df2c8cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the preprocessing function\n",
    "is_test = False  # Global var to control if enciphering for test dataset or not\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Add prefix to the sentences, tokenize the text, and set the labels\"\"\"\n",
    "    # Create lists of data of instructions w/ ciphered text and the corresponding plaintext\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for doc in examples[\"document\"]:\n",
    "        # NOTE: non-test data shift is only positive to ensure that negative wraparounds don't show examples \n",
    "        # of effectively other positive shifts (e.g. shift of -2 = shift of 24)\n",
    "        if is_test:\n",
    "            shift = random.randint(-25, 25)\n",
    "        else:\n",
    "            shift = random.randint(0, 13)\n",
    "        prefix = f\"Use a Caesar cipher with shift {shift} to decipher the following text: \"\n",
    "        text = format_text(doc)\n",
    "        inputs.append(prefix + caesar_cipher(text, shift, True))\n",
    "        targets.append(text)\n",
    "\n",
    "    # Tokenize\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True)\n",
    "    labels = tokenizer(text_target=targets, max_length=128, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e9854e46-2612-4a35-99f6-c18dfccb1019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "926974c82ab34d9687552105c83726bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38040 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bd543a5fe414384bf002a31bc2425e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "574c09589bd74a10bc7ab0c66ab278be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1951 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training/validation dataset encipherment\n",
    "tokenized_dataset = small_dataset.map(preprocess_function, batched=True, remove_columns=[\"document\", \"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7dabd433-82b8-429f-aaa6-b8b3b95e6320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "742ec34ba5084086bd56023359faa964",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1951 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test datset encipherment (different than above because want to test additional unseen during training cipher shifts)\n",
    "is_test = True\n",
    "tokenized_dataset['test'] = small_dataset['test'].map(preprocess_function, batched=True, remove_columns=[\"document\", \"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6ba6c2a5-5b1c-4ccf-b07b-0f2ee3b1efd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92b2f20eb4914a5fb3cae23092f5e7c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/38040 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9887390e21194130ae942e162878128f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9844eef6ad3f4a34a1b2214fc4ddc880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1951 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save dataset to disk\n",
    "tokenized_dataset.save_to_disk(dataset_dict_path='/home/as6734/langgen_class_project/data/half_caesar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e667a8-ec2a-4333-9ff2-16579c6f99c9",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "985fcda6-ba84-4759-892c-1aa5f465082a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from datasets import load_from_disk\n",
    "import torch\n",
    "from evaluate import load\n",
    "import numpy as np\n",
    "from transformers import T5Tokenizer, DataCollatorForSeq2Seq\n",
    "from transformers import T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d7edcda3-b54c-469c-9c36-44c6399ddb7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check CUDA working\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7fcd073b-6b7a-4792-8184-56e630d91ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer, model, and data collator\n",
    "MODEL_NAME = \"google/flan-t5-base\"\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME, model_max_length=128)\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e1934a59-150f-4825-b153-da2a380e5ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved dataset\n",
    "small_dataset = load_from_disk('/home/as6734/langgen_class_project/data/half_caesar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a880597b-c759-4b01-aab3-49bf785c5776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation metric wrapper function using character error rate (CER)\n",
    "cer = load(\"cer\")\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    # decode preds and labels\n",
    "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    return {'cer': cer.compute(predictions=decoded_preds, references=decoded_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "131f35e7-8960-453f-ae1c-738ca7da112c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Parameters\n",
    "L_RATE = 3e-4\n",
    "BATCH_SIZE = 8\n",
    "PER_DEVICE_EVAL_BATCH = 8\n",
    "WEIGHT_DECAY = 0.01\n",
    "SAVE_TOTAL_LIM = 3\n",
    "NUM_EPOCHS = 3\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "   output_dir='/home/as6734/langgen_class_project/results/half_caesar',\n",
    "   evaluation_strategy=\"epoch\",\n",
    "   learning_rate=L_RATE,\n",
    "   per_device_train_batch_size=BATCH_SIZE,\n",
    "   per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH,\n",
    "   weight_decay=WEIGHT_DECAY,\n",
    "   save_total_limit=SAVE_TOTAL_LIM,\n",
    "   num_train_epochs=NUM_EPOCHS,\n",
    "   predict_with_generate=True,\n",
    "   push_to_hub=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bed4161d-a351-4d0c-a344-43281699363a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "   model=model,\n",
    "   args=training_args,\n",
    "   train_dataset=small_dataset[\"train\"],\n",
    "   eval_dataset=small_dataset[\"validation\"],\n",
    "   tokenizer=tokenizer,\n",
    "   data_collator=data_collator,\n",
    "   compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1713e87f-813d-426f-8940-a09af0fc1689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14265' max='14265' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14265/14265 1:53:15, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Cer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.487200</td>\n",
       "      <td>1.306866</td>\n",
       "      <td>0.670902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.960200</td>\n",
       "      <td>0.954309</td>\n",
       "      <td>0.628372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.779400</td>\n",
       "      <td>0.892751</td>\n",
       "      <td>0.614901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/langgen/lib/python3.11/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/langgen/lib/python3.11/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/langgen/lib/python3.11/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=14265, training_loss=1.331752453258152, metrics={'train_runtime': 6796.2868, 'train_samples_per_second': 16.792, 'train_steps_per_second': 2.099, 'total_flos': 1.953612726534144e+16, 'train_loss': 1.331752453258152, 'epoch': 3.0})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b453e10-cca3-4791-99cb-c5bd5a3cf34a",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "33a09964-974c-49cc-875d-ef619a265ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from evaluate import load\n",
    "from datasets import load_from_disk\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9cf23be0-3b71-4b5b-98b4-b8435a4e20a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model from fine tuning checkpoint\n",
    "last_checkpoint = '/home/as6734/langgen_class_project/results/half_caesar/checkpoint-14000'\n",
    "finetuned_model = T5ForConditionalGeneration.from_pretrained(last_checkpoint)\n",
    "tokenizer = T5Tokenizer.from_pretrained(last_checkpoint)\n",
    "finetuned_model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "edc4fe11-e65e-48ec-91da-c5688a3f9edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_from_disk('/home/as6734/langgen_class_project/data/half_caesar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4bbe99c7-f87f-4428-8dbe-edce74802534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Input: 'Use a Caesar cipher with shift -12 to decipher the following text: xodob g bsq qcfd obr iby qcadihsf qcfd ct hvs ibwhsr ghohsg gowr ksrbsgrom hvsm vor oufssr hc xcwb tcfqsg wb gidsfqcadi</s>'\n",
      "Model Output: '<pad>oder v rogu bor vyud vyud vyudyuv juu vyudyuv won gusudhu vyudyuv won comfortably while ji jiangfuu won kuomintang pt vyudyuv vyudyuv won the unk unk unk unk vyudyuv vyudyuv vyudyuv'\n",
      "True Output: 'japan s nec corp and unk computer corp of the united states said wednesday they had agreed to join forces in supercomputer sales</s>'\n"
     ]
    }
   ],
   "source": [
    "# Qualitative example within test dataset\n",
    "input_string = tokenizer.decode(dataset['test'][0]['input_ids'])\n",
    "print(f\"Model Input: '{input_string}'\")\n",
    "input_ids = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "outputs = finetuned_model.generate(input_ids.to(\"cuda\"), max_length=128)\n",
    "print(f\"Model Output: '{tokenizer.decode(outputs[0])}'\")\n",
    "print(f\"True Output: '{tokenizer.decode(dataset['test'][0]['labels'])}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3338cd90-2d28-4c4b-81c7-58900d7fa27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Input: 'Use a Caesar cipher with shift 4 to decipher the following text: mwveip tvitevih wyrhec jsv tvmqi qmrmwxiv cmxdleo vefmr w wxexi jyrivep almgl ampp fi exxirhih fc e lswx sj asvph piehivw mrgpyhmrk yw</s>'\n",
      "Model Output: '<pad> israel promised sunday for prime minister yitzhak rabin s state funeral which will be attended by a host of world leaders including us president george w bush and israeli prime minister ariel sharon</s>'\n",
      "True Output: 'israel prepared sunday for prime minister yitzhak rabin s state funeral which will be attended by a host of world leaders including us president bill clinton and the jordanian and egyptian heads of state</s>'\n"
     ]
    }
   ],
   "source": [
    "# Qualitative example within test dataset and shift seen in training\n",
    "input_string = tokenizer.decode(dataset['test'][7]['input_ids'])\n",
    "print(f\"Model Input: '{input_string}'\")\n",
    "input_ids = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "outputs = finetuned_model.generate(input_ids.to(\"cuda\"), max_length=128)\n",
    "print(f\"Model Output: '{tokenizer.decode(outputs[0])}'\")\n",
    "print(f\"True Output: '{tokenizer.decode(dataset['test'][7]['labels'])}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ecb6a02b-2e33-47ba-ba83-e229fb846fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Input: 'Use a Caesar cipher with shift -21 to decipher the following text: ymj gfsp tk ofufs fuujfqji yt knsfshnfq rfwpjyx yt wjrfns hfqr kwnifd ktqqtbnsl ymj zx ijhnxnts yt twijw ifnbf gf</s>'\n",
      "Model Output: '<pad> the bank of japan appealed to financial markets to remain calm friday following the us decision to order daiwa bank to sell its stake in a unk bank in a bid to restructure the financial system</s>'\n",
      "True Output: 'the bank of japan appealed to financial markets to remain calm friday following the us decision to order daiwa bank ltd to close its us operations</s>'\n"
     ]
    }
   ],
   "source": [
    "# Qualitative example within test dataset and equivalent negative shift seen in training\n",
    "input_string = tokenizer.decode(dataset['test'][4]['input_ids'])\n",
    "print(f\"Model Input: '{input_string}'\")\n",
    "input_ids = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "outputs = finetuned_model.generate(input_ids.to(\"cuda\"), max_length=128)\n",
    "print(f\"Model Output: '{tokenizer.decode(outputs[0])}'\")\n",
    "print(f\"True Output: '{tokenizer.decode(dataset['test'][4]['labels'])}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a636d9ca-26f1-49c9-9df6-35bda5baa2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load error metrics\n",
    "cer = load(\"cer\")\n",
    "bleu = load(\"bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6c44ce91-d503-491d-a253-dbcfc26e175d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5b8c880bb0646b6affb58a1626874ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1951 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Metrics over entire test dataset\n",
    "predictions = []\n",
    "references = []\n",
    "in_predictions = []\n",
    "in_references = []\n",
    "out_predictions = []\n",
    "out_references = []\n",
    "for i in tqdm(range(len(dataset['test']))):\n",
    "    input_string = tokenizer.decode(dataset['test'][i]['input_ids'])\n",
    "    # Get the shift value\n",
    "    ls = re.findall(r'\\d+', input_string)\n",
    "    shift = int(ls[0])\n",
    "    if '-' in input_string:\n",
    "        shift *= -1\n",
    "\n",
    "    # Get pred and reference text\n",
    "    input_ids = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    outputs = finetuned_model.generate(input_ids.to(\"cuda\"), max_length=128)\n",
    "    pred = re.sub(\"[\\<\\[].*?[\\>\\]]\", \"\", tokenizer.decode(outputs[0])).strip()\n",
    "    ref = re.sub(\"[\\<\\[].*?[\\>\\]]\", \"\", tokenizer.decode(dataset['test'][i]['labels'])).strip()\n",
    "\n",
    "    # Add to corresponding list for metrics\n",
    "    if len(ref) > 1 and len(pred) > 1:\n",
    "        predictions.append(pred)\n",
    "        references.append(ref)\n",
    "        # Is within training distribution of shifts (if you include equivalent negatives)\n",
    "        if shift % 26 <= 13:\n",
    "            in_predictions.append(pred)\n",
    "            in_references.append(ref)\n",
    "        # Shift not seen in training data\n",
    "        else:\n",
    "            out_predictions.append(pred)\n",
    "            out_references.append(ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3526a8fd-9091-447f-a57e-ee327ac29a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CER Score: 0.627223594033461\n",
      "{'bleu': 0.2500741925531097, 'precisions': [0.328927611132451, 0.2632311474224568, 0.22696018735362997, 0.19901610017889088], 'brevity_penalty': 1.0, 'length_ratio': 1.0879079132317746, 'translation_length': 57274, 'reference_length': 52646}\n"
     ]
    }
   ],
   "source": [
    "# All test data metrics\n",
    "cer_score = cer.compute(predictions=predictions, references=references)\n",
    "print(f'CER Score: {cer_score}')\n",
    "results = bleu.compute(predictions=predictions, references=references)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "beacce35-4b08-4b67-8059-5d3b324b21a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CER Score: 0.34732339810540236\n",
      "{'bleu': 0.5034863882115634, 'precisions': [0.656563108195776, 0.5446100831522961, 0.47224171539961013, 0.4160230875538574], 'brevity_penalty': 0.9779727416472119, 'length_ratio': 0.9782118177972078, 'translation_length': 27746, 'reference_length': 28364}\n"
     ]
    }
   ],
   "source": [
    "# Only seen in training\n",
    "cer_score = cer.compute(predictions=in_predictions, references=in_references)\n",
    "print(f'CER Score: {cer_score}')\n",
    "results = bleu.compute(predictions=in_predictions, references=in_references)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d4e4f5b4-918d-4d38-8f24-ea9cc84c8559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CER Score: 0.9535392948013943\n",
      "{'bleu': 0.0, 'precisions': [0.02106475209970198, 0.0008034653811220568, 3.606853020739405e-05, 0.0], 'brevity_penalty': 1.0, 'length_ratio': 1.2160448068528127, 'translation_length': 29528, 'reference_length': 24282}\n"
     ]
    }
   ],
   "source": [
    "# Never seen in training\n",
    "cer_score = cer.compute(predictions=out_predictions, references=out_references)\n",
    "print(f'CER Score: {cer_score}')\n",
    "results = bleu.compute(predictions=out_predictions, references=out_references)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f98ac4-214b-4490-9ed2-bada8bb611c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
