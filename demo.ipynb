{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c33ea2dc-f5bb-4739-a8e8-359c244e8992",
   "metadata": {},
   "source": [
    "# Demo Notebook\n",
    "### Purpose: Show example prompting and generations of fine tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5677ee36-f8f8-4eb3-b073-8eda01df4052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "import random\n",
    "from datasets import load_from_disk\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d36a968-1152-432e-afa7-d50e4caf7cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enciphering/deciphering helpers\n",
    "char_to_num = {\n",
    "    'a': 0,\n",
    "    'b': 1,\n",
    "    'c': 2,\n",
    "    'd': 3,\n",
    "    'e': 4,\n",
    "    'f': 5,\n",
    "    'g': 6,\n",
    "    'h': 7,\n",
    "    'i': 8,\n",
    "    'j': 9,\n",
    "    'k': 10,\n",
    "    'l': 11,\n",
    "    'm': 12,\n",
    "    'n': 13,\n",
    "    'o': 14,\n",
    "    'p': 15,\n",
    "    'q': 16,\n",
    "    'r': 17,\n",
    "    's': 18,\n",
    "    't': 19,\n",
    "    'u': 20,\n",
    "    'v': 21,\n",
    "    'w': 22,\n",
    "    'x': 23,\n",
    "    'y': 24,\n",
    "    'z': 25,\n",
    "}\n",
    "\n",
    "\n",
    "# Remove all non alphabet text except spaces\n",
    "def format_text(text):\n",
    "    plaintext = re.sub(r'[^A-Za-z ]+', '', text)\n",
    "    return plaintext.lower()\n",
    "\n",
    "\n",
    "# NOTE: shift can be negative (left) or positive (right)\n",
    "# If encode=True, encipher text, otherwise decipher\n",
    "def caesar_cipher(original, shift, encode):\n",
    "    if encode:\n",
    "        myshift = shift\n",
    "    else:\n",
    "        myshift = shift * -1\n",
    "    newtext = ''\n",
    "    for i in original:\n",
    "        if i == ' ':  # Preserve spaces\n",
    "            newtext += ' '\n",
    "        else:\n",
    "            newnum = (char_to_num[i] + myshift) % 26\n",
    "            newchar = list(char_to_num.keys())[list(char_to_num.values()).index(newnum)]\n",
    "            newtext += newchar\n",
    "    return newtext\n",
    "\n",
    "\n",
    "# Randomly enciphers on word level\n",
    "def random_caesar_encipher(original, shift, partial):\n",
    "    newtext = ''\n",
    "    cipher_new = False\n",
    "    for i in original:\n",
    "        if i == ' ':  # Preserve spaces\n",
    "            newtext += ' '\n",
    "            # Check if next word should be enciphered or not\n",
    "            if random.uniform(0, 1) <= partial:\n",
    "                cipher_new = True\n",
    "            else:\n",
    "                cipher_new = False\n",
    "        else:\n",
    "            # Only encipher if word has been selected\n",
    "            if cipher_new:\n",
    "                newnum = (char_to_num[i] + shift) % 26\n",
    "                newchar = list(char_to_num.keys())[list(char_to_num.values()).index(newnum)]\n",
    "                newtext += newchar\n",
    "            # Keep old char in place\n",
    "            else:\n",
    "                newtext += i\n",
    "    return newtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecb57c0f-7e5e-4570-9d2f-75799ef27eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of model/data\n",
    "exp_name = 'caesar'\n",
    "# exp_name = 'caesar_long'\n",
    "# exp_name = 'half_caesar'\n",
    "# exp_name = 'partial_caesar'\n",
    "# exp_name = 'partial_word_caesar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e44943c4-b3dd-4e36-92da-86d5509ce33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/langgen/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "dataset = load_from_disk(f\"/home/as6734/langgen_class_project/data/{exp_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b73d25c4-d7e5-437e-919b-262c53de4164",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "last_checkpoint = f\"/home/as6734/langgen_class_project/results/{exp_name}/checkpoint-14000\"\n",
    "finetuned_model = T5ForConditionalGeneration.from_pretrained(last_checkpoint)\n",
    "tokenizer = T5Tokenizer.from_pretrained(last_checkpoint)\n",
    "finetuned_model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff6ac878-c999-4975-82c4-e3963bdfa384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Input: 'Use a Caesar cipher with shift 17 to decipher the following text: argre j evt tfig reu leb tfdglkvi tfig fw kyv lezkvu jkrkvj jrzu nvuevjurp kyvp yru rxivvu kf afze wfitvj ze jlgvitfdglkvi jrcvj</s>'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/langgen/lib/python3.11/site-packages/transformers/models/t5/tokenization_t5.py:303: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Output: '<pad> japan s new corp and unk computer corp of the united states said wednesday they had agreed to join forces in supercomputer sales</s>'\n",
      "True Output: 'japan s nec corp and unk computer corp of the united states said wednesday they had agreed to join forces in supercomputer sales</s>'\n"
     ]
    }
   ],
   "source": [
    "# Example using test set\n",
    "test_set_index = 0\n",
    "input_string = tokenizer.decode(dataset['test'][test_set_index]['input_ids'])\n",
    "print(f\"Model Input: '{input_string}'\")\n",
    "input_ids = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "outputs = finetuned_model.generate(input_ids.to(\"cuda\"), max_length=128)\n",
    "print(f\"Model Output: '{tokenizer.decode(outputs[test_set_index])}'\")\n",
    "print(f\"True Output: '{tokenizer.decode(dataset['test'][test_set_index]['labels'])}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67de3b15-09ce-45cc-9aff-7c8741c235d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stripped Message: 'try to translate a simple message just like this one'\n",
      "Enciphered Message: 'usz up usbotmbuf b tjnqmf nfttbhf kvtu mjlf uijt pof'\n",
      "Model Input: 'Use a Caesar cipher with shift 1 to decipher the following text: usz up usbotmbuf b tjnqmf nfttbhf kvtu mjlf uijt pof'\n",
      "Model Output: '<pad> try to transform a sizable message just like this one</s>'\n",
      "True output: 'try to translate a simple message just like this one'\n"
     ]
    }
   ],
   "source": [
    "# Example using user defined text\n",
    "shift = 1\n",
    "message = \"Try to translate a simple message just like this one!\"\n",
    "\n",
    "message = format_text(message)\n",
    "print(f\"Stripped Message: '{message}'\")\n",
    "\n",
    "ctext = caesar_cipher(message, shift, encode=True)\n",
    "print(f\"Enciphered Message: '{ctext}'\")\n",
    "\n",
    "input_text = f\"Use a Caesar cipher with shift {shift} to decipher the following text: {ctext}\"\n",
    "print(f\"Model Input: '{input_text}'\")\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "outputs = finetuned_model.generate(input_ids.to(\"cuda\"), max_length=128)\n",
    "print(f\"Model Output: '{tokenizer.decode(outputs[0])}'\")\n",
    "print(f\"True output: '{message}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c37e98-feb1-48e4-a8d8-d35179d95c86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
