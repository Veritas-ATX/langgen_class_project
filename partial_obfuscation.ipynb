{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11df56f6-4a91-4cf5-a1c6-fff2a14b349c",
   "metadata": {},
   "source": [
    "# Data Prep/Training/Evaluation Notebook for Caesar Cipher w/ Partially Obfuscated Text\n",
    "### Purpose is to download, process, and write out instruction tuning examples for Caesar ciphers\n",
    "### Data has had a percentage of characters enciphered at random from the original message\n",
    "### Then train a model and evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3910137e-7fdc-43f8-ba68-15f1967113ac",
   "metadata": {},
   "source": [
    "## Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f957f460-d90d-4d7e-8643-3b23975f2bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "import random\n",
    "from datasets import load_from_disk, load_dataset\n",
    "from transformers import T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1148a79a-1086-4dfd-b5bd-3528bd520b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enciphering/deciphering helpers\n",
    "char_to_num = {\n",
    "    'a': 0,\n",
    "    'b': 1,\n",
    "    'c': 2,\n",
    "    'd': 3,\n",
    "    'e': 4,\n",
    "    'f': 5,\n",
    "    'g': 6,\n",
    "    'h': 7,\n",
    "    'i': 8,\n",
    "    'j': 9,\n",
    "    'k': 10,\n",
    "    'l': 11,\n",
    "    'm': 12,\n",
    "    'n': 13,\n",
    "    'o': 14,\n",
    "    'p': 15,\n",
    "    'q': 16,\n",
    "    'r': 17,\n",
    "    's': 18,\n",
    "    't': 19,\n",
    "    'u': 20,\n",
    "    'v': 21,\n",
    "    'w': 22,\n",
    "    'x': 23,\n",
    "    'y': 24,\n",
    "    'z': 25,\n",
    "}\n",
    "\n",
    "\n",
    "# Remove all non alphabet text except spaces\n",
    "def format_text(text):\n",
    "    plaintext = re.sub(r'[^A-Za-z ]+', '', text)\n",
    "    return plaintext.lower()\n",
    "\n",
    "\n",
    "# NOTE: shift can be negative (left) or positive (right)\n",
    "# If encode=True, encipher text, otherwise decipher\n",
    "def caesar_cipher(original, shift, encode):\n",
    "    if encode:\n",
    "        myshift = shift\n",
    "    else:\n",
    "        myshift = shift * -1\n",
    "    newtext = ''\n",
    "    for i in original:\n",
    "        if i == ' ':  # Preserve spaces\n",
    "            newtext += ' '\n",
    "        else:\n",
    "            newnum = (char_to_num[i] + myshift) % 26\n",
    "            newchar = list(char_to_num.keys())[list(char_to_num.values()).index(newnum)]\n",
    "            newtext += newchar\n",
    "    return newtext\n",
    "\n",
    "\n",
    "# Randomly enciphers\n",
    "def random_caesar_encipher(original, shift, partial):\n",
    "    newtext = ''\n",
    "    for i in original:\n",
    "        if i == ' ':  # Preserve spaces\n",
    "            newtext += ' '\n",
    "        else:\n",
    "            # Generate rand float [0, 1) and encipher char only if value <= partial\n",
    "            if random.uniform(0, 1) <= partial:\n",
    "                newnum = (char_to_num[i] + shift) % 26\n",
    "                newchar = list(char_to_num.keys())[list(char_to_num.values()).index(newnum)]\n",
    "                newtext += newchar\n",
    "            # If rand value not large enough, keep old char in place\n",
    "            else:\n",
    "                newtext += i\n",
    "    return newtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "235befd6-2dea-4fea-8178-b5ba845eef80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/langgen/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['document', 'summary'],\n",
       "        num_rows: 3803957\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['document', 'summary'],\n",
       "        num_rows: 189651\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['document', 'summary'],\n",
       "        num_rows: 1951\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download gigaword dataset from huggingface\n",
    "DATA_NAME = \"gigaword\"\n",
    "gigaword = load_dataset(DATA_NAME)\n",
    "gigaword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54f638d2-fc76-44c2-8018-ac2008b42939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['document', 'summary'],\n",
       "        num_rows: 38040\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['document', 'summary'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['document', 'summary'],\n",
       "        num_rows: 1951\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reduce the length of GIGAWORD dataset to make training faster\n",
    "small_dataset = gigaword.filter(lambda example, idx: idx < 38040, with_indices=True)\n",
    "small_dataset['validation'] = small_dataset['validation'].filter(lambda example, idx: idx < 2000, with_indices=True)\n",
    "small_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfdd9501-acad-4cf6-9dca-9cbaabf42880",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Get the base model tokenizer\n",
    "MODEL_NAME = \"google/flan-t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6dcd3b9-c10b-4883-b793-a254df2c8cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the preprocessing function\n",
    "partial_frac = 0.5  # The fraction of text to be enciphered at random per image\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Add prefix to the sentences, tokenize the text, and set the labels\"\"\"\n",
    "    # Create lists of data of instructions w/ ciphered text and the corresponding plaintext\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for doc in examples[\"document\"]:\n",
    "        shift = random.randint(-25, 25)\n",
    "        prefix = f\"Use a Caesar cipher with shift {shift} to decipher the following text: \"\n",
    "        text = format_text(doc)\n",
    "        inputs.append(prefix + random_caesar_encipher(text, shift, partial_frac))\n",
    "        targets.append(text)\n",
    "\n",
    "    # Tokenize\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True)\n",
    "    labels = tokenizer(text_target=targets, max_length=128, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9854e46-2612-4a35-99f6-c18dfccb1019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "533ac008458d4c81a2ceefadedf9144c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38040 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b569c1892ed43479988b5bc50d5f435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9b2dcd2211d47f4938001eda0b00cf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1951 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preprocess/tokenize data\n",
    "tokenized_dataset = small_dataset.map(preprocess_function, batched=True, remove_columns=[\"document\", \"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ba6c2a5-5b1c-4ccf-b07b-0f2ee3b1efd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3b1e33b62a44d9d8c4021c797abeba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/38040 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37f064692b8a4f628747e6d715450b5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "829c8b1532c64a73b2d674880c1bd192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1951 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save dataset to disk\n",
    "tokenized_dataset.save_to_disk(dataset_dict_path='/home/as6734/langgen_class_project/data/partial_caesar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e667a8-ec2a-4333-9ff2-16579c6f99c9",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "985fcda6-ba84-4759-892c-1aa5f465082a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from datasets import load_from_disk\n",
    "import torch\n",
    "from evaluate import load\n",
    "import numpy as np\n",
    "from transformers import T5Tokenizer, DataCollatorForSeq2Seq\n",
    "from transformers import T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7edcda3-b54c-469c-9c36-44c6399ddb7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check CUDA working\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fcd073b-6b7a-4792-8184-56e630d91ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer, model, and data collator\n",
    "MODEL_NAME = \"google/flan-t5-base\"\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME, model_max_length=128)\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1934a59-150f-4825-b153-da2a380e5ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved dataset\n",
    "small_dataset = load_from_disk('/home/as6734/langgen_class_project/data/partial_caesar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a880597b-c759-4b01-aab3-49bf785c5776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation metric wrapper function using character error rate (CER)\n",
    "cer = load(\"cer\")\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    # decode preds and labels\n",
    "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    return {'cer': cer.compute(predictions=decoded_preds, references=decoded_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "131f35e7-8960-453f-ae1c-738ca7da112c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Parameters\n",
    "L_RATE = 3e-4\n",
    "BATCH_SIZE = 8\n",
    "PER_DEVICE_EVAL_BATCH = 8\n",
    "WEIGHT_DECAY = 0.01\n",
    "SAVE_TOTAL_LIM = 3\n",
    "NUM_EPOCHS = 3\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "   output_dir='/home/as6734/langgen_class_project/results/partial_caesar',\n",
    "   evaluation_strategy=\"epoch\",\n",
    "   learning_rate=L_RATE,\n",
    "   per_device_train_batch_size=BATCH_SIZE,\n",
    "   per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH,\n",
    "   weight_decay=WEIGHT_DECAY,\n",
    "   save_total_limit=SAVE_TOTAL_LIM,\n",
    "   num_train_epochs=NUM_EPOCHS,\n",
    "   predict_with_generate=True,\n",
    "   push_to_hub=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bed4161d-a351-4d0c-a344-43281699363a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "   model=model,\n",
    "   args=training_args,\n",
    "   train_dataset=small_dataset[\"train\"],\n",
    "   eval_dataset=small_dataset[\"validation\"],\n",
    "   tokenizer=tokenizer,\n",
    "   data_collator=data_collator,\n",
    "   compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1713e87f-813d-426f-8940-a09af0fc1689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14265' max='14265' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14265/14265 1:53:26, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Cer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.557400</td>\n",
       "      <td>1.376153</td>\n",
       "      <td>0.713500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.070400</td>\n",
       "      <td>1.058310</td>\n",
       "      <td>0.668120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.866500</td>\n",
       "      <td>0.962397</td>\n",
       "      <td>0.656876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/langgen/lib/python3.11/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/langgen/lib/python3.11/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/langgen/lib/python3.11/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=14265, training_loss=1.3368738986716286, metrics={'train_runtime': 6806.8836, 'train_samples_per_second': 16.765, 'train_steps_per_second': 2.096, 'total_flos': 1.95498224123904e+16, 'train_loss': 1.3368738986716286, 'epoch': 3.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b453e10-cca3-4791-99cb-c5bd5a3cf34a",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33a09964-974c-49cc-875d-ef619a265ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from evaluate import load\n",
    "from datasets import load_from_disk\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9cf23be0-3b71-4b5b-98b4-b8435a4e20a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model from fine tuning checkpoint\n",
    "last_checkpoint = '/home/as6734/langgen_class_project/results/partial_caesar/checkpoint-14000'\n",
    "finetuned_model = T5ForConditionalGeneration.from_pretrained(last_checkpoint)\n",
    "tokenizer = T5Tokenizer.from_pretrained(last_checkpoint)\n",
    "finetuned_model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "edc4fe11-e65e-48ec-91da-c5688a3f9edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_from_disk('/home/as6734/langgen_class_project/data/partial_caesar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4bbe99c7-f87f-4428-8dbe-edce74802534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Input: 'Use a Caesar cipher with shift -23 to decipher the following text: jdsan s qef coup dqg xqn comsuwer cous rf the unlthd swatev sald zednesgab theb hdd agreeg wr join irrces iq vxshrfopsxweu vdlev</s>'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/langgen/lib/python3.11/site-packages/transformers/models/t5/tokenization_t5.py:303: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Output: '<pad> japan s new coup and unk commander both of the united states said wednesday they had agreed to join forces in switzerland saeco</s>'\n",
      "True Output: 'japan s nec corp and unk computer corp of the united states said wednesday they had agreed to join forces in supercomputer sales</s>'\n"
     ]
    }
   ],
   "source": [
    "# Qualitative example 1\n",
    "input_string = tokenizer.decode(dataset['test'][0]['input_ids'])\n",
    "print(f\"Model Input: '{input_string}'\")\n",
    "input_ids = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "outputs = finetuned_model.generate(input_ids.to(\"cuda\"), max_length=128)\n",
    "print(f\"Model Output: '{tokenizer.decode(outputs[0])}'\")\n",
    "print(f\"True Output: '{tokenizer.decode(dataset['test'][0]['labels'])}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3338cd90-2d28-4c4b-81c7-58900d7fa27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Input: 'Use a Caesar cipher with shift -23 to decipher the following text: tke vrl oankaq joyhrnpeqt rq zhgqesdab dnqoxnchd wke forsxue of joveunphqw vchooov zith ipmhgldwh eiiecw ds d mioiwaub cdmsdlgn djdinvt</s>'\n",
      "Model Output: '<pad> the sri lankan government on wednesday announced the formation of government schools with private schools as a military campaign against terrorism</s>'\n",
      "True Output: 'the sri lankan government on wednesday announced the closure of government schools with immediate effect as a military campaign against tamil separatists escalated in the north of the country</s>'\n"
     ]
    }
   ],
   "source": [
    "# Qualitative example 2\n",
    "input_string = tokenizer.decode(dataset['test'][1]['input_ids'])\n",
    "print(f\"Model Input: '{input_string}'\")\n",
    "input_ids = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "outputs = finetuned_model.generate(input_ids.to(\"cuda\"), max_length=128)\n",
    "print(f\"Model Output: '{tokenizer.decode(outputs[0])}'\")\n",
    "print(f\"True Output: '{tokenizer.decode(dataset['test'][1]['labels'])}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ecb6a02b-2e33-47ba-ba83-e229fb846fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Input: 'Use a Caesar cipher with shift 20 to decipher the following text: piliwe arlysnyx zivy uhnihowlyal jrotymtyrs nhormduy afnel nhyy siogbn no xcslopt louxiha iz u fryhwh untalcncc lyseulwh anx supjly vesmyl a mjokesguh zor</s>'\n",
      "Model Output: '<pad> police arrested five antigovernment protesters thursday after they sought to disrupt holding of a french antiterror newspaper and supply video a spokesman for the spokesman said</s>'\n",
      "True Output: 'police arrested five antinuclear protesters thursday after they sought to disrupt loading of a french antarctic research and supply vessel a spokesman for the protesters said</s>'\n"
     ]
    }
   ],
   "source": [
    "# Qualitative example 3\n",
    "input_string = tokenizer.decode(dataset['test'][2]['input_ids'])\n",
    "print(f\"Model Input: '{input_string}'\")\n",
    "input_ids = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "outputs = finetuned_model.generate(input_ids.to(\"cuda\"), max_length=128)\n",
    "print(f\"Model Output: '{tokenizer.decode(outputs[0])}'\")\n",
    "print(f\"True Output: '{tokenizer.decode(dataset['test'][2]['labels'])}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a636d9ca-26f1-49c9-9df6-35bda5baa2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load error metrics\n",
    "cer = load(\"cer\")\n",
    "bleu = load(\"bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c9169d3f-2e36-4ee9-b5ef-b036211e49d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ea41698a7c34c61b751b9c14b1031ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1951 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56f05424751941709ef768401dbe37f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1951 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "655fb47118f540f38cce8595a065fc83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1951 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Construct test dataset w/ 25% encipherment percentage\n",
    "g_test = gigaword['test']\n",
    "partial_frac = 0.25\n",
    "low_test = g_test.map(preprocess_function, batched=True, remove_columns=[\"document\", \"summary\"])\n",
    "partial_frac = 0.75\n",
    "med_test = g_test.map(preprocess_function, batched=True, remove_columns=[\"document\", \"summary\"])\n",
    "partial_frac = 0.9\n",
    "high_test = g_test.map(preprocess_function, batched=True, remove_columns=[\"document\", \"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c44ce91-d503-491d-a253-dbcfc26e175d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b859db6257fe4d32977827d55cc95c5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1951 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Metrics over test datasets\n",
    "predictions_25 = []\n",
    "predictions_50 = []\n",
    "predictions_75 = []\n",
    "predictions_90 = []\n",
    "references = []\n",
    "for i in tqdm(range(len(dataset['test']))):\n",
    "    # Partial = 0.5 (same as training)\n",
    "    input_string = tokenizer.decode(dataset['test'][i]['input_ids'])\n",
    "    input_ids = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    outputs = finetuned_model.generate(input_ids.to(\"cuda\"), max_length=128)\n",
    "    pred_50 = re.sub(\"[\\<\\[].*?[\\>\\]]\", \"\", tokenizer.decode(outputs[0])).strip()\n",
    "\n",
    "    # Partial = 0.25\n",
    "    input_string = tokenizer.decode(low_test[i]['input_ids'])\n",
    "    input_ids = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    outputs = finetuned_model.generate(input_ids.to(\"cuda\"), max_length=128)\n",
    "    pred_25 = re.sub(\"[\\<\\[].*?[\\>\\]]\", \"\", tokenizer.decode(outputs[0])).strip()\n",
    "\n",
    "    # Partial = 0.75\n",
    "    input_string = tokenizer.decode(med_test[i]['input_ids'])\n",
    "    input_ids = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    outputs = finetuned_model.generate(input_ids.to(\"cuda\"), max_length=128)\n",
    "    pred_75 = re.sub(\"[\\<\\[].*?[\\>\\]]\", \"\", tokenizer.decode(outputs[0])).strip()\n",
    "\n",
    "    # Partial = 0.9\n",
    "    input_string = tokenizer.decode(high_test[i]['input_ids'])\n",
    "    input_ids = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    outputs = finetuned_model.generate(input_ids.to(\"cuda\"), max_length=128)\n",
    "    pred_90 = re.sub(\"[\\<\\[].*?[\\>\\]]\", \"\", tokenizer.decode(outputs[0])).strip()\n",
    "\n",
    "    # Reference is the same so only need to do this once!\n",
    "    ref = re.sub(\"[\\<\\[].*?[\\>\\]]\", \"\", tokenizer.decode(dataset['test'][i]['labels'])).strip()\n",
    "\n",
    "    # Add to corresponding list for metrics\n",
    "    if len(ref) > 1 and len(pred_50) > 1:\n",
    "        predictions_25.append(pred_25)\n",
    "        predictions_50.append(pred_50)\n",
    "        predictions_75.append(pred_75)\n",
    "        predictions_90.append(pred_90)\n",
    "        references.append(ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d3503758-583d-4a15-a28d-c7a66ab45471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CER Score: 0.1966337431969361\n",
      "{'bleu': 0.5787161801952778, 'precisions': [0.7663049774105108, 0.6334363213225263, 0.5363919096620676, 0.46009961502490376], 'brevity_penalty': 0.9836847770016477, 'length_ratio': 0.9838164342970026, 'translation_length': 51794, 'reference_length': 52646}\n"
     ]
    }
   ],
   "source": [
    "# Test data metrics for partial_frac = 0.25\n",
    "cer_score = cer.compute(predictions=predictions_25, references=references)\n",
    "print(f'CER Score: {cer_score}')\n",
    "results = bleu.compute(predictions=predictions_25, references=references)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6ea91d08-b224-431a-8037-ccf0fff7f4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CER Score: 0.3540049889135255\n",
      "{'bleu': 0.399262339741416, 'precisions': [0.6206869025524356, 0.4619904898357524, 0.3652077969841854, 0.29712582691743245], 'brevity_penalty': 0.9506306928884153, 'length_ratio': 0.9518102040041029, 'translation_length': 50109, 'reference_length': 52646}\n"
     ]
    }
   ],
   "source": [
    "# Test data metrics for partial_frac = 0.5 (same as training)\n",
    "cer_score = cer.compute(predictions=predictions_50, references=references)\n",
    "print(f'CER Score: {cer_score}')\n",
    "results = bleu.compute(predictions=predictions_50, references=references)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "86fdd76b-81d8-4f64-8c7d-81f4dbfe214b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CER Score: 0.4312953537593227\n",
      "{'bleu': 0.31911857326521176, 'precisions': [0.5519291509258686, 0.38596717644275236, 0.2939803384656821, 0.2343445719246381], 'brevity_penalty': 0.9168543699378167, 'length_ratio': 0.920126885233446, 'translation_length': 48441, 'reference_length': 52646}\n"
     ]
    }
   ],
   "source": [
    "# Test data metrics for partial_frac = 0.75\n",
    "cer_score = cer.compute(predictions=predictions_75, references=references)\n",
    "print(f'CER Score: {cer_score}')\n",
    "results = bleu.compute(predictions=predictions_75, references=references)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aab92662-a5ef-4be0-8b22-b12d2ddc5482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CER Score: 0.4620036283007458\n",
      "{'bleu': 0.2955605861343824, 'precisions': [0.5222224501097278, 0.3561646762236418, 0.2680350337634552, 0.21059618071727992], 'brevity_penalty': 0.9233352378300547, 'length_ratio': 0.9261292405880789, 'translation_length': 48757, 'reference_length': 52646}\n"
     ]
    }
   ],
   "source": [
    "# Test data metrics for partial_frac = 0.9\n",
    "cer_score = cer.compute(predictions=predictions_90, references=references)\n",
    "print(f'CER Score: {cer_score}')\n",
    "results = bleu.compute(predictions=predictions_90, references=references)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a36ffb-a4e1-48c7-9e77-01df40e28e7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
