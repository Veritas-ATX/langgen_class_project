{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c21e811-7b03-4978-8d50-6a4855843523",
   "metadata": {},
   "source": [
    "# Evaluation Notebook\n",
    "### Purpose of this notebook is to evaluate models for the decipher task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6618a52e-3591-47da-bac4-b65a48f1925f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from evaluate import load\n",
    "from datasets import load_from_disk\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad50dcc3-8a4d-4b36-9b72-8a388e241382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Check CUDA working\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86b233ad-1b01-407c-a155-92d968685549",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Load model from fine tuning checkpoint\n",
    "last_checkpoint = '/home/as6734/langgen_class_project/results/caesar/checkpoint-14000'\n",
    "finetuned_model = T5ForConditionalGeneration.from_pretrained(last_checkpoint)\n",
    "tokenizer = T5Tokenizer.from_pretrained(last_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02be4add-4f07-459b-9dde-19a270693bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/langgen/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = load_from_disk('/home/as6734/langgen_class_project/data/caesar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9aaf0638-0df5-4105-a727-9b566791c5eb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetuned_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b9733a-2845-44b5-9160-d1f4fe84c5aa",
   "metadata": {},
   "source": [
    "### Qualitative Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f1db7fd-f23b-4d87-bcb5-73bd2374b65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> helen mo nao is austin</s>\n",
      "True output: hello my name is austin\n"
     ]
    }
   ],
   "source": [
    "# Qualitative example outside of test dataset\n",
    "input_text = \"Use a Caesar cipher with shift 25 to decipher the following text: gdkkn lx mzld hr ztrshm\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "outputs = finetuned_model.generate(input_ids.to(\"cuda\"), max_length=128)\n",
    "print(tokenizer.decode(outputs[0]))\n",
    "print('True output: hello my name is austin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2bee404-c4e5-4c5c-8f23-2f99eb9bd33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Input: 'Use a Caesar cipher with shift 17 to decipher the following text: argre j evt tfig reu leb tfdglkvi tfig fw kyv lezkvu jkrkvj jrzu nvuevjurp kyvp yru rxivvu kf afze wfitvj ze jlgvitfdglkvi jrcvj</s>'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/langgen/lib/python3.11/site-packages/transformers/models/t5/tokenization_t5.py:303: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Output: '<pad> japan s new corp and unk computer corp of the united states said wednesday they had agreed to join forces in supercomputer sales</s>'\n",
      "True Output: 'japan s nec corp and unk computer corp of the united states said wednesday they had agreed to join forces in supercomputer sales</s>'\n"
     ]
    }
   ],
   "source": [
    "# Qualitative example within test dataset\n",
    "input_string = tokenizer.decode(dataset['test'][0]['input_ids'])\n",
    "print(f\"Model Input: '{input_string}'\")\n",
    "input_ids = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "outputs = finetuned_model.generate(input_ids.to(\"cuda\"), max_length=128)\n",
    "print(f\"Model Output: '{tokenizer.decode(outputs[0])}'\")\n",
    "print(f\"True Output: '{tokenizer.decode(dataset['test'][0]['labels'])}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d0529f1-b8a3-44f5-86be-21c5dfe5cd07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Input: 'Use a Caesar cipher with shift 1 to decipher the following text: uif tsj mbolbo hpwfsonfou po xfeoftebz boopvodfe uif dmptvsf pg hpwfsonfou tdippmt xjui jnnfejbuf fggfdu bt b njmjubsz dbnqbjho bhbjot</s>'\n",
      "Model Output: '<pad> the sri lankan government on wednesday announced the closure of government schools with immediate effect as a military campaign against tamil tiger rebels continued</s>'\n",
      "True Output: 'the sri lankan government on wednesday announced the closure of government schools with immediate effect as a military campaign against tamil separatists escalated in the north of the country</s>'\n"
     ]
    }
   ],
   "source": [
    "# Qualitative example within test dataset\n",
    "input_string = tokenizer.decode(dataset['test'][1]['input_ids'])\n",
    "print(f\"Model Input: '{input_string}'\")\n",
    "input_ids = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "outputs = finetuned_model.generate(input_ids.to(\"cuda\"), max_length=128)\n",
    "print(f\"Model Output: '{tokenizer.decode(outputs[0])}'\")\n",
    "print(f\"True Output: '{tokenizer.decode(dataset['test'][1]['labels'])}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2059600b-7b76-4f8d-a6bd-86f6331febb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Input: 'Use a Caesar cipher with shift -17 to decipher the following text: yxurln jaanbcnm oren jwcrwdlunja yaxcnbcnab cqdabmjh jocna cqnh bxdpqc cx mrbadyc uxjmrwp xo j oanwlq jwcjalcrl anbnjalq jwm bdy</s>'\n",
      "Model Output: '<pad> police arrested five antigovernment protesters thursday after they sought to disrupt running of a french anticrime research and support center in the capital london police said</s>'\n",
      "True Output: 'police arrested five antinuclear protesters thursday after they sought to disrupt loading of a french antarctic research and supply vessel a spokesman for the protesters said</s>'\n"
     ]
    }
   ],
   "source": [
    "# Qualitative example within test dataset\n",
    "input_string = tokenizer.decode(dataset['test'][2]['input_ids'])\n",
    "print(f\"Model Input: '{input_string}'\")\n",
    "input_ids = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "outputs = finetuned_model.generate(input_ids.to(\"cuda\"), max_length=128)\n",
    "print(f\"Model Output: '{tokenizer.decode(outputs[0])}'\")\n",
    "print(f\"True Output: '{tokenizer.decode(dataset['test'][2]['labels'])}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd7ee60b-a07c-4ea1-a3b2-d109b8557d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Input: 'Use a Caesar cipher with shift 17 to decipher the following text: wrtkfip fiuvij wfi drelwrtklivu xffuj ifjv gvitvek ze jvgkvdsvi kyv tfddvitv uvgrikdvek jrzu yviv kylijurp</s>'\n",
      "Model Output: '<pad> factory orders for manufactured goods rose percent in september the commerce department said here thursday</s>'\n",
      "True Output: 'factory orders for manufactured goods rose percent in september the commerce department said here thursday</s>'\n"
     ]
    }
   ],
   "source": [
    "# Qualitative example within test dataset\n",
    "input_string = tokenizer.decode(dataset['test'][3]['input_ids'])\n",
    "print(f\"Model Input: '{input_string}'\")\n",
    "input_ids = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "outputs = finetuned_model.generate(input_ids.to(\"cuda\"), max_length=128)\n",
    "print(f\"Model Output: '{tokenizer.decode(outputs[0])}'\")\n",
    "print(f\"True Output: '{tokenizer.decode(dataset['test'][3]['labels'])}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ded0ca70-38b0-4b5b-8360-ee67a3f85866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Input: 'Use a Caesar cipher with shift 10 to decipher the following text: dro lkxu yp tkzkx kzzokvon dy psxkxmskv wkbuodc dy bowksx mkvw pbsnki pyvvygsxq dro ec nomscsyx dy ybnob nksgk lkxu vdn </s>'\n",
      "Model Output: '<pad> the bank of japan appealed to financial markets to remain calm friday following the us decision to order daiwa bank ltd to sell its assets in a bid to boost liquidity</s>'\n",
      "True Output: 'the bank of japan appealed to financial markets to remain calm friday following the us decision to order daiwa bank ltd to close its us operations</s>'\n"
     ]
    }
   ],
   "source": [
    "# Qualitative example within test dataset\n",
    "input_string = tokenizer.decode(dataset['test'][4]['input_ids'])\n",
    "print(f\"Model Input: '{input_string}'\")\n",
    "input_ids = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "outputs = finetuned_model.generate(input_ids.to(\"cuda\"), max_length=128)\n",
    "print(f\"Model Output: '{tokenizer.decode(outputs[0])}'\")\n",
    "print(f\"True Output: '{tokenizer.decode(dataset['test'][4]['labels'])}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28097095-2bb9-47ed-8d88-a31b9dc43a56",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef40c9b9-fb0e-41fe-bfd9-87b5c0236b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cer = load(\"cer\")\n",
    "bleu = load(\"bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36b2bc33-24d4-43e9-86e7-bc88d558e2c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28735632183908044"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Character Error Rate example\n",
    "predictions = ['police arrested five antigovernment protesters thursday after they sought to disrupt running of a french anticrime research and support center in the capital london police said']\n",
    "references = ['police arrested five antinuclear protesters thursday after they sought to disrupt loading of a french antarctic research and supply vessel a spokesman for the protesters said']\n",
    "cer_score = cer.compute(predictions=predictions, references=references)\n",
    "cer_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20119892-8e72-4511-ae9a-b912eb37f287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.34756561191481233, 'precisions': [0.6538461538461539, 0.44, 0.2916666666666667, 0.17391304347826086], 'brevity_penalty': 1.0, 'length_ratio': 1.0, 'translation_length': 26, 'reference_length': 26}\n"
     ]
    }
   ],
   "source": [
    "# BLEU example\n",
    "predictions = ['police arrested five antigovernment protesters thursday after they sought to disrupt running of a french anticrime research and support center in the capital london police said']\n",
    "references = ['police arrested five antinuclear protesters thursday after they sought to disrupt loading of a french antarctic research and supply vessel a spokesman for the protesters said']\n",
    "results = bleu.compute(predictions=predictions, references=references)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34746af1-386c-4637-aa23-22a656ab880d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "549b1f2f3df9401fa3db57c65b6288ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1951 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.3006733773432776"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CER over test dataset\n",
    "predictions = []\n",
    "references = []\n",
    "for i in tqdm(range(len(dataset['test']))):\n",
    "    input_string = tokenizer.decode(dataset['test'][i]['input_ids'])\n",
    "    input_ids = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    outputs = finetuned_model.generate(input_ids.to(\"cuda\"), max_length=128)\n",
    "    pred = re.sub(\"[\\<\\[].*?[\\>\\]]\", \"\", tokenizer.decode(outputs[0])).strip()\n",
    "    ref = re.sub(\"[\\<\\[].*?[\\>\\]]\", \"\", tokenizer.decode(dataset['test'][i]['labels'])).strip()\n",
    "    if len(ref) > 1 and len(pred) > 1:\n",
    "        predictions.append(pred)\n",
    "        references.append(ref)\n",
    "cer_score = cer.compute(predictions=predictions, references=references)\n",
    "cer_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61a59def-3100-49fa-8141-c96affa7ad7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.5635670573146343, 'precisions': [0.704759646731077, 0.6107309454821939, 0.5501198424927238, 0.5002790365426256], 'brevity_penalty': 0.9606284763178234, 'length_ratio': 0.9613835808988337, 'translation_length': 50613, 'reference_length': 52646}\n"
     ]
    }
   ],
   "source": [
    "# BLEU over test data\n",
    "results = bleu.compute(predictions=predictions, references=references)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23922f4c-9526-4eab-8168-260aac61c75c",
   "metadata": {},
   "source": [
    "## Zero Shot Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "962cfaac-5ccb-4dcc-ab20-cab29ba9aba4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "base_model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\", device_map=\"auto\")\n",
    "base_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "430f49cc-7860-47d8-9317-ae320c04265e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d6d073f950f4c0284d1cc274b0fa777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1951 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CER Score: 0.9279454878314194\n",
      "{'bleu': 0.00028700751106246983, 'precisions': [0.018694304533126694, 0.01641489235161175, 0.014958962139263966, 0.014094846571722214], 'brevity_penalty': 0.01799558505241833, 'length_ratio': 0.19929732442762826, 'translation_length': 10324, 'reference_length': 51802}\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "references = []\n",
    "for i in tqdm(range(len(dataset['test']))):\n",
    "    input_string = base_tokenizer.decode(dataset['test'][i]['input_ids'])\n",
    "    input_ids = base_tokenizer(input_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    outputs = base_model.generate(input_ids.to(\"cuda\"), max_length=128)\n",
    "    pred = re.sub(\"[\\<\\[].*?[\\>\\]]\", \"\", base_tokenizer.decode(outputs[0])).strip()\n",
    "    ref = re.sub(\"[\\<\\[].*?[\\>\\]]\", \"\", base_tokenizer.decode(dataset['test'][i]['labels'])).strip()\n",
    "    if len(ref) > 1 and len(pred) > 1:\n",
    "        predictions.append(pred)\n",
    "        references.append(ref)\n",
    "cer_score = cer.compute(predictions=predictions, references=references)\n",
    "print(f'CER Score: {cer_score}')\n",
    "results = bleu.compute(predictions=predictions, references=references)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d1edde-b722-4aef-9d0d-4acce9acd2e4",
   "metadata": {},
   "source": [
    "### Evaluate Long Training Fine Tuned Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "990a3787-82ca-4252-8771-89478f1f7026",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model from fine tuning checkpoint\n",
    "last_checkpoint = '/home/as6734/langgen_class_project/results/caesar_long/checkpoint-71000'\n",
    "long_model = T5ForConditionalGeneration.from_pretrained(last_checkpoint)\n",
    "long_tokenizer = T5Tokenizer.from_pretrained(last_checkpoint)\n",
    "long_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9c37e69-00ba-44cb-8aac-2f44c2b19ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Input: 'Use a Caesar cipher with shift -17 to decipher the following text: yxurln jaanbcnm oren jwcrwdlunja yaxcnbcnab cqdabmjh jocna cqnh bxdpqc cx mrbadyc uxjmrwp xo j oanwlq jwcjalcrl anbnjalq jwm bdy</s>'\n",
      "Model Output: '<pad> police arrested five antinuclear protesters thursday after they sought to disrupt loading of a french antarctic research and supply vessel in the arctic a police spokesman said</s>'\n",
      "True Output: 'police arrested five antinuclear protesters thursday after they sought to disrupt loading of a french antarctic research and supply vessel a spokesman for the protesters said</s>'\n"
     ]
    }
   ],
   "source": [
    "# Qualitative example within test dataset\n",
    "input_string = long_tokenizer.decode(dataset['test'][2]['input_ids'])\n",
    "print(f\"Model Input: '{input_string}'\")\n",
    "input_ids = long_tokenizer(input_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "outputs = long_model.generate(input_ids.to(\"cuda\"), max_length=128)\n",
    "print(f\"Model Output: '{long_tokenizer.decode(outputs[0])}'\")\n",
    "print(f\"True Output: '{long_tokenizer.decode(dataset['test'][2]['labels'])}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "438a6f64-2c92-441b-99f8-97031261b336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fef585cc1494c5f94f5e931a5026454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1951 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CER Score: 0.2357923049788349\n",
      "{'bleu': 0.6950594722407475, 'precisions': [0.7982921856390173, 0.747893332212579, 0.7206694120739507, 0.6972323879231473], 'brevity_penalty': 0.9391677498239391, 'length_ratio': 0.9409451810204004, 'translation_length': 49537, 'reference_length': 52646}\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "references = []\n",
    "for i in tqdm(range(len(dataset['test']))):\n",
    "    input_string = long_tokenizer.decode(dataset['test'][i]['input_ids'])\n",
    "    input_ids = long_tokenizer(input_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    outputs = long_model.generate(input_ids.to(\"cuda\"), max_length=128)\n",
    "    pred = re.sub(\"[\\<\\[].*?[\\>\\]]\", \"\", long_tokenizer.decode(outputs[0])).strip()\n",
    "    ref = re.sub(\"[\\<\\[].*?[\\>\\]]\", \"\", long_tokenizer.decode(dataset['test'][i]['labels'])).strip()\n",
    "    if len(ref) > 1 and len(pred) > 1:\n",
    "        predictions.append(pred)\n",
    "        references.append(ref)\n",
    "cer_score = cer.compute(predictions=predictions, references=references)\n",
    "print(f'CER Score: {cer_score}')\n",
    "results = bleu.compute(predictions=predictions, references=references)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4b0b9d-2b12-43e7-958b-f21550b4fcc3",
   "metadata": {},
   "source": [
    "### Caesar Ciphers with Out of Training Shifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d57ab3bb-e630-4990-8f52-0638cfe33765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> hollywood cc nasa is okay</s>\n",
      "True output: hello my name is austin\n"
     ]
    }
   ],
   "source": [
    "# Qualitative example outside of test dataset w/ small training model\n",
    "input_text = \"Use a Caesar cipher with shift 30 to decipher the following text: lipps qc reqi mw eywxmr\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "outputs = finetuned_model.generate(input_ids.to(\"cuda\"), max_length=128)\n",
    "print(tokenizer.decode(outputs[0]))\n",
    "print('True output: hello my name is austin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ddb0be61-db36-4866-a72b-baf8416a6525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> helen mc name is augustin</s>\n",
      "True output: hello my name is austin\n"
     ]
    }
   ],
   "source": [
    "# Qualitative example outside of test dataset w/ long training model\n",
    "input_text = \"Use a Caesar cipher with shift 30 to decipher the following text: lipps qc reqi mw eywxmr\"\n",
    "input_ids = long_tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "outputs = long_model.generate(input_ids.to(\"cuda\"), max_length=128)\n",
    "print(long_tokenizer.decode(outputs[0]))\n",
    "print('True output: hello my name is austin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a43b3ba2-b56f-4285-b1dd-84620396ea2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> mw eywxmr</s>\n",
      "True output: hello my name is austin\n"
     ]
    }
   ],
   "source": [
    "# Qualitative example outside of test dataset w/ base model\n",
    "input_text = \"Use a Caesar cipher with shift 30 to decipher the following text: lipps qc reqi mw eywxmr\"\n",
    "input_ids = base_tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "outputs = base_model.generate(input_ids.to(\"cuda\"), max_length=128)\n",
    "print(base_tokenizer.decode(outputs[0]))\n",
    "print('True output: hello my name is austin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cef5e2cb-aacb-4582-89a4-bae6af868d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "# Enciphering/deciphering helpers\n",
    "char_to_num = {\n",
    "    'a': 0,\n",
    "    'b': 1,\n",
    "    'c': 2,\n",
    "    'd': 3,\n",
    "    'e': 4,\n",
    "    'f': 5,\n",
    "    'g': 6,\n",
    "    'h': 7,\n",
    "    'i': 8,\n",
    "    'j': 9,\n",
    "    'k': 10,\n",
    "    'l': 11,\n",
    "    'm': 12,\n",
    "    'n': 13,\n",
    "    'o': 14,\n",
    "    'p': 15,\n",
    "    'q': 16,\n",
    "    'r': 17,\n",
    "    's': 18,\n",
    "    't': 19,\n",
    "    'u': 20,\n",
    "    'v': 21,\n",
    "    'w': 22,\n",
    "    'x': 23,\n",
    "    'y': 24,\n",
    "    'z': 25,\n",
    "}\n",
    "\n",
    "\n",
    "# Remove all non alphabet text except spaces\n",
    "def format_text(text):\n",
    "    plaintext = re.sub(r'[^A-Za-z ]+', '', text)\n",
    "    return plaintext.lower()\n",
    "\n",
    "\n",
    "# NOTE: shift can be negative (left) or positive (right)\n",
    "# If encode=True, encipher text, otherwise decipher\n",
    "def caesar_cipher(original, shift, encode):\n",
    "    if encode:\n",
    "        myshift = shift\n",
    "    else:\n",
    "        myshift = shift * -1\n",
    "    newtext = ''\n",
    "    for i in original:\n",
    "        if i == ' ':  # Preserve spaces\n",
    "            newtext += ' '\n",
    "        else:\n",
    "            newnum = (char_to_num[i] + myshift) % 26\n",
    "            newchar = list(char_to_num.keys())[list(char_to_num.values()).index(newnum)]\n",
    "            newtext += newchar\n",
    "    return newtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d98e4739-8c00-4a5f-bc3b-9b0b9b31de2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3864a588ccb4b25ac1930bcf1f60fe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1951 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CER Score: 0.14365991763898422\n",
      "{'bleu': 0.6710583413749363, 'precisions': [0.7983968909400049, 0.7062516769519721, 0.6325528474758968, 0.5685464762033715], 'brevity_penalty': 1.0, 'length_ratio': 1.0230604840713682, 'translation_length': 20585, 'reference_length': 20121}\n"
     ]
    }
   ],
   "source": [
    "# metrics for outside of test dataset w/ short training model\n",
    "predictions = []\n",
    "references = []\n",
    "for i in tqdm(range(len(dataset['test']))):\n",
    "    shift = random.choice(list(set([x for x in range(-100, 101)]) - set([x for x in range(-25, 26)])))\n",
    "    prefix = f\"Use a Caesar cipher with shift {shift} to decipher the following text: \"\n",
    "    plaintext = tokenizer.decode(dataset['test'][i]['labels'])\n",
    "    plaintext = re.sub(\"[\\<\\[].*?[\\>\\]]\", \"\", plaintext).strip()\n",
    "    if len(plaintext) + len(prefix) > 128:\n",
    "        over = len(plaintext) + len(prefix) - 128\n",
    "        plaintext = plaintext[:-over]\n",
    "    input_string = prefix + caesar_cipher(plaintext, shift, True)\n",
    "    \n",
    "    input_ids = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    outputs = finetuned_model.generate(input_ids.to(\"cuda\"), max_length=128)\n",
    "    pred = re.sub(\"[\\<\\[].*?[\\>\\]]\", \"\", tokenizer.decode(outputs[0])).strip()\n",
    "    ref = plaintext.strip()\n",
    "    if len(ref) > 1 and len(pred) > 1:\n",
    "        predictions.append(pred)\n",
    "        references.append(ref)\n",
    "cer_score = cer.compute(predictions=predictions, references=references)\n",
    "print(f'CER Score: {cer_score}')\n",
    "results = bleu.compute(predictions=predictions, references=references)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1e86851f-12e3-42b5-8c4d-ff0f2cde1859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab956d9de8264562b5aa5f2520eb5718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1951 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CER Score: 0.5232511258846236\n",
      "{'bleu': 0.5544523032577391, 'precisions': [0.6168406871000337, 0.5768565248738284, 0.5367384901565648, 0.4948267917731328], 'brevity_penalty': 1.0, 'length_ratio': 1.4747665408305186, 'translation_length': 29690, 'reference_length': 20132}\n"
     ]
    }
   ],
   "source": [
    "# metrics for outside of test dataset w/ long training model\n",
    "predictions = []\n",
    "references = []\n",
    "for i in tqdm(range(len(dataset['test']))):\n",
    "    shift = random.choice(list(set([x for x in range(-100, 101)]) - set([x for x in range(-25, 26)])))\n",
    "    prefix = f\"Use a Caesar cipher with shift {shift} to decipher the following text: \"\n",
    "    plaintext = long_tokenizer.decode(dataset['test'][i]['labels'])\n",
    "    plaintext = re.sub(\"[\\<\\[].*?[\\>\\]]\", \"\", plaintext).strip()\n",
    "    if len(plaintext) + len(prefix) > 128:\n",
    "        over = len(plaintext) + len(prefix) - 128\n",
    "        plaintext = plaintext[:-over]\n",
    "    input_string = prefix + caesar_cipher(plaintext, shift, True)\n",
    "    \n",
    "    input_ids = long_tokenizer(input_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    outputs = long_model.generate(input_ids.to(\"cuda\"), max_length=128)\n",
    "    pred = re.sub(\"[\\<\\[].*?[\\>\\]]\", \"\", long_tokenizer.decode(outputs[0])).strip()\n",
    "    ref = plaintext.strip()\n",
    "    if len(ref) > 1 and len(pred) > 1:\n",
    "        predictions.append(pred)\n",
    "        references.append(ref)\n",
    "cer_score = cer.compute(predictions=predictions, references=references)\n",
    "print(f'CER Score: {cer_score}')\n",
    "results = bleu.compute(predictions=predictions, references=references)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3b0c13fb-55b6-400f-b4fb-4e2015b84bab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e517174580c43c791e8ac8c7dc6dc88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1951 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CER Score: 0.9598249661025903\n",
      "{'bleu': 0.00287753338434403, 'precisions': [0.034015966678236725, 0.034420289855072464, 0.030470914127423823, 0.02831923501287238], 'brevity_penalty': 0.0907620477004479, 'length_ratio': 0.2941596896058811, 'translation_length': 5762, 'reference_length': 19588}\n"
     ]
    }
   ],
   "source": [
    "# metrics for outside of test dataset w/ base model\n",
    "predictions = []\n",
    "references = []\n",
    "for i in tqdm(range(len(dataset['test']))):\n",
    "    shift = random.choice(list(set([x for x in range(-100, 101)]) - set([x for x in range(-25, 26)])))\n",
    "    prefix = f\"Use a Caesar cipher with shift {shift} to decipher the following text: \"\n",
    "    plaintext = base_tokenizer.decode(dataset['test'][i]['labels'])\n",
    "    plaintext = re.sub(\"[\\<\\[].*?[\\>\\]]\", \"\", plaintext).strip()\n",
    "    if len(plaintext) + len(prefix) > 128:\n",
    "        over = len(plaintext) + len(prefix) - 128\n",
    "        plaintext = plaintext[:-over]\n",
    "    input_string = prefix + caesar_cipher(plaintext, shift, True)\n",
    "    \n",
    "    input_ids = base_tokenizer(input_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    outputs = base_model.generate(input_ids.to(\"cuda\"), max_length=128)\n",
    "    pred = re.sub(\"[\\<\\[].*?[\\>\\]]\", \"\", base_tokenizer.decode(outputs[0])).strip()\n",
    "    ref = plaintext.strip()\n",
    "    if len(ref) > 1 and len(pred) > 1:\n",
    "        predictions.append(pred)\n",
    "        references.append(ref)\n",
    "cer_score = cer.compute(predictions=predictions, references=references)\n",
    "print(f'CER Score: {cer_score}')\n",
    "results = bleu.compute(predictions=predictions, references=references)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b86d90-cfb6-42b0-a0bf-e26384f6a733",
   "metadata": {},
   "source": [
    "### In Context Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3d07dd-47d0-4946-beb5-d34c470fefec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e591b82-e774-45ba-bb97-6f748dc619f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112155bf-5232-4761-bc21-343f4f141722",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e433218d-f565-4895-b7ff-c49d856661bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3803957\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 189651\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1951\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d8b0a277-89ba-40d7-9699-b19b6b3772a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c116d55ecef04ea7b4e7349de64977c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1951 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inputs = []\n",
    "refs = []\n",
    "for i in tqdm(range(len(dataset['test']))):\n",
    "    input_string = tokenizer.decode(dataset['test'][i]['input_ids'])\n",
    "    input_string = re.sub(\"[\\<\\[].*?[\\>\\]]\", \"\", input_string).strip()\n",
    "    inputs.append(len(input_string))\n",
    "    ref_string = tokenizer.decode(dataset['test'][i]['labels'])\n",
    "    ref_string = re.sub(\"[\\<\\[].*?[\\>\\]]\", \"\", ref_string).strip()    \n",
    "    refs.append(len(ref_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1401d51c-4271-4ba1-9d49-48035e77a3ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183.46386468477704"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(inputs) / len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b01a1466-4bef-4ce0-88b5-37236446df87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162.73910814966683"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(refs) / len(refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7e292de7-8d5a-410b-85b4-85d179c9539d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix = f\"Use a Caesar cipher with shift 2 to decipher the following text: \"\n",
    "len(prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0eae4370-bb2f-4271-97a0-4309734ba16a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Use a Caesar cipher with shift 17 to decipher the following text: argre j evt tfig reu leb tfdglkvi tfig fw kyv lezkvu jkrkvj jrzu nvuevjurp kyvp yru rxivvu kf afze wfitvj ze jlgvitfdglkvi jrcvj'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_string = tokenizer.decode(dataset['test'][0]['input_ids'])\n",
    "input_string = re.sub(\"[\\<\\[].*?[\\>\\]]\", \"\", input_string).strip()\n",
    "input_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "78829328-84b4-4c9b-99c6-fe2a1833e698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('Use a Caesar cipher with shift 17 to decipher the following text:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2716b002-fdbb-4761-afac-a7d4854629e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183.46386468477704"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.mean(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fc7a8701-2469-4c51-9e86-a9f5a6f6326f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a544a5b3ab84b879cf807180ab33e33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3803957 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191.5129\n",
      "172.9865\n"
     ]
    }
   ],
   "source": [
    "inputs = []\n",
    "refs = []\n",
    "count = 0\n",
    "for i in tqdm(range(len(dataset['train']))):\n",
    "    input_string = tokenizer.decode(dataset['train'][i]['input_ids'])\n",
    "    input_string = re.sub(\"[\\<\\[].*?[\\>\\]]\", \"\", input_string).strip()\n",
    "    inputs.append(len(input_string))\n",
    "    ref_string = tokenizer.decode(dataset['train'][i]['labels'])\n",
    "    ref_string = re.sub(\"[\\<\\[].*?[\\>\\]]\", \"\", ref_string).strip()    \n",
    "    refs.append(len(ref_string))\n",
    "    count += 1\n",
    "    if count >= 10000:\n",
    "        break\n",
    "print(np.mean(inputs))\n",
    "print(np.mean(refs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "487b41d7-4af2-427d-ba13-497a32d80d2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6448b93e453e4aae9c82977fb2c3fb6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/189651 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191.4036\n",
      "172.3924\n"
     ]
    }
   ],
   "source": [
    "inputs = []\n",
    "refs = []\n",
    "count = 0\n",
    "for i in tqdm(range(len(dataset['validation']))):\n",
    "    input_string = tokenizer.decode(dataset['validation'][i]['input_ids'])\n",
    "    input_string = re.sub(\"[\\<\\[].*?[\\>\\]]\", \"\", input_string).strip()\n",
    "    inputs.append(len(input_string))\n",
    "    ref_string = tokenizer.decode(dataset['validation'][i]['labels'])\n",
    "    ref_string = re.sub(\"[\\<\\[].*?[\\>\\]]\", \"\", ref_string).strip()    \n",
    "    refs.append(len(ref_string))\n",
    "    count += 1\n",
    "    if count >= 10000:\n",
    "        break\n",
    "print(np.mean(inputs))\n",
    "print(np.mean(refs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf0b0b7-791d-48f9-a556-312a7dcd60a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
